
name: Deploy and Trigger Airflow DAG

on:
  push:
    paths:
      - dags/**
      - docker-compose.yml
      - Dockerfile
      - entrypoint.sh
      - requirements.txt
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      # Step 1: Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v3

      # Step 2: Set up Docker
      - name: Set up Docker
        uses: docker/setup-buildx-action@v2

      - name: Set Docker socket permissions
        run: |
          sudo chmod 666 /var/run/docker.sock  # Change permissions to allow access
          ls -l /var/run/docker.sock  # Optional: Check permissions after changing
      
      # Step 3: Install Docker Compose
      - name: Install Docker Compose
        run: |
          sudo apt-get update
          sudo apt-get install -y docker-compose

      # Step 4: Build and start Airflow using Docker Compose
      - name: Build and Start Airflow
        run: |
          docker-compose down || true
          docker-compose up -d --build

      - name: Mount Docker socket and start Airflow container
        run: |
          docker run -d \
            -v /var/run/docker.sock:/var/run/docker.sock \
            --name airflow-container \
            airflow

      - name: Debug List Running Containers
        run: |
          echo "Listing running containers for debugging:"
          docker ps -a
        shell: bash

      - name: Set Permissions for Writable Folders
        run: |
          echo "Setting permissions for writable folders..."
          chmod -R 777 logs mlruns models outputs
        shell: bash

      # Step 6: Trigger the data_pipeline.py DAG
      - name: Trigger data_pipeline.py DAG
        run: |
          docker exec airflow-container airflow dags trigger data_pipeline

      
      - name: Debug Airflow Logs
        run: |
          echo "Fetching Airflow webserver logs for debugging:"
          docker logs airflow-container || true
        shell: bash

      
